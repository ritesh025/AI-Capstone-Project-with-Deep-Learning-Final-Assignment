{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6498861f-52ef-4ba3-bbbe-9259792a610f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf6b95-f975-42df-b63b-f59078b5cd51",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Comparative Analysis of Keras and PyTorch Models </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f6f80-948d-4eb5-bd73-e328fc644dda",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b5aad",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul> \n",
    "\n",
    "1. Prepare data, load and evaluate Keras model.\n",
    "2. Prepare data, load and evaluate PyTorch model.\n",
    "3. Compute multiple performance metrics including accuracy, precision, recall, and f1-score.\n",
    "4. Visualize receiver operating characteristic (ROC) curves.\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639ce32-38b5-410f-9c8f-7186738b8a38",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will compare the performance of the Keras-based and the PyTorch based convolutional neural network (CNN) models using various evaluation metrics.  Common metrics include:\n",
    "\n",
    "- **Accuracy**: Measures how often the model is correct overall. A higher value means more total predictions are correct.\n",
    "\n",
    "- **Precision**: Measures how many predicted positives are actually correct. A higher value means fewer false positives (incorrectly predicted positives).\n",
    "\n",
    "- **Recall**: Measures how many real positives the model finds. A higher value means fewer false negatives (missed positive cases).\n",
    "\n",
    "- **F1 Score**: Tells us about the balance between precision and recall. A higher value means a better trade-off between precision and recall.\n",
    "\n",
    "- **ROC-AUC**: Measures the model’s ability to distinguish classes. A higher value reflects a model that can better distinguish between classes at all probability thresholds.\n",
    "\n",
    "\n",
    "For all these metrics, the model should aim for values as close to 1.0 (or 100%) as possible. Lower values indicate poorer model performance. There are exceptions for some metrics in other settings (like various loss functions, where lower is better), but for these standard classification metrics, higher is always better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6219bc4-772b-4a74-88b1-4efe292b53b7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<font size = 3> \n",
    "    \n",
    "1. [Data download and extraction](#Data-download-and-extraction)\n",
    "2. [Package installation](#Package-installation)\n",
    "3. [Library imports and setup](#Library-imports-and-setup)\n",
    "4. [Evaluation metrics](#Evaluation-metrics)\n",
    "    1. [Accuracy](#1.-Accuracy)\n",
    "    2. [Precision](#2.-Precision)\n",
    "    3. [Recall](#3.-Recall-(sensitivity-or-true-positive-rate))\n",
    "    4. [F1 score](#4.-F1-score)\n",
    "    5. [Confusion matrix](#5.-Confusion-matrix)\n",
    "    6. [ROC-AUC](#6.-ROC-AUC-(Receiver-operating-characteristic---Area-under-curve))\n",
    "6. [Import the evaluation metrics](#Import-the-evaluation-metrics)\n",
    "7. [Model paths and download](#Model-paths-and-download)\n",
    "8. [Dataset path and parameters](#Dataset-path-and-parameters)\n",
    "9. [PyTorch model evaluation and prediction](#PyTorch-model-evaluation-and-prediction)\n",
    "10. [PyTorch metrics reporting](#PyTorch-metrics-reporting)\n",
    "11. [Keras model evaluation and prediction](#Keras-model-evaluation-and-prediction)\n",
    "12. [Keras metrics reporting](#Keras-metrics-reporting)\n",
    "13. [ROC curve plotting](#ROC-curve-plotting)\n",
    "14. [Comparing model performance](#Comparing-model-performance)\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18f62d",
   "metadata": {},
   "source": [
    "## Data download and extraction\n",
    "We begin by downloading the dataset for evaluation of the models.\n",
    "Here, you declare:\n",
    "1. The dataset URL from where the dataset would be downloaded.\n",
    "2. The dataset downloading primary function, based on `skillsnetwork` library.\n",
    "3. The dataset fallback downloading function, based on regular `http` downloading functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9f0820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aee711c9f2b4d999d2cecdd6e1562d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f04e347081440594bde53a410751fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "data_dir = \".\"\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\"\n",
    "\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1f18c",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "\n",
    "Install the required basic Python packages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7ca672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.3 ms, sys: 44.1 ms, total: 115 ms\n",
      "Wall time: 23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28739c-8b88-4fc1-901b-7a3c24f8dd4f",
   "metadata": {},
   "source": [
    "### Install PyTorch library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a97ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.7.0\n",
      "  Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting filelock (from torch==2.7.0)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (75.8.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.7.0)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (3.1.5)\n",
      "Collecting fsspec (from torch==2.7.0)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0)\n",
      "  Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.7.0)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0) (3.0.2)\n",
      "Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m954.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m186.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.20.0 fsspec-2025.10.0 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0 triton-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 6.49 s, sys: 1.51 s, total: 7.99 s\n",
      "Wall time: 7min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torch==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348399a-ee14-4050-a752-96f367f21b12",
   "metadata": {},
   "source": [
    "### Install PyTorch helper libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a20f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision==0.22\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (1.26.0)\n",
      "Requirement already satisfied: torch==2.7.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (12.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision==0.22) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision==0.22) (3.0.2)\n",
      "Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 141 ms, sys: 43.3 ms, total: 184 ms\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torchvision==0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd033f37-165f-41b8-90e2-a2439f035575",
   "metadata": {},
   "source": [
    "### Install tensorflow library for Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.19\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.19)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.19)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow==2.19)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.19)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.19)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.19)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.19)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow==2.19)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.19)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.19)\n",
      "  Downloading wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.19)\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow==2.19)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow==2.19)\n",
      "  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.26.0)\n",
      "Collecting h5py>=3.11.0 (from tensorflow==2.19)\n",
      "  Downloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow==2.19)\n",
      "  Downloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.19) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow==2.19)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow==2.19)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow==2.19)\n",
      "  Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow==2.19)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow==2.19)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow==2.19)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow==2.19)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Downloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install tensorflow==2.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c8f13-46b3-4e8e-a988-3af92750dfee",
   "metadata": {},
   "source": [
    "### Install SkLearn library for evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c4ade",
   "metadata": {},
   "source": [
    "## Library imports and setup\n",
    "\n",
    "Import essential libraries for data manipulation, visualization, and suppresses warnings for cleaner notebook output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fcdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c7a98",
   "metadata": {},
   "source": [
    "### PyTorch library imports\n",
    "\n",
    "Import core PyTorch modules for model building, optimization, data loading, and functional utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Imported libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0a554",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras library imports\n",
    "\n",
    "These imports set the environment variables to reduce TensorFlow logging noise and imports Keras modules for model building and training. They detect GPU availability for device assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99804321",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "device = \"gpu\" if gpu_list != [] else \"cpu\"\n",
    "print(f\"Device available for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae515f",
   "metadata": {},
   "source": [
    "## Evaluation metrics \n",
    "\n",
    "The following metrics are used for evaluation of various AI/ML models:\n",
    "    \n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- Confusion matrix\n",
    "- Receiver Operating Characteristic - Area Under Curve (ROC-AUC)\n",
    "\n",
    "You can read about their calculation methods and their significance for model performance below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e4b91-1b94-4563-a099-6a811a328f70",
   "metadata": {},
   "source": [
    "### 1. Accuracy\n",
    "\n",
    "**Definition:**\n",
    "Accuracy is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. In other words, it measures how often the classifier is correct overall.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\\]\n",
    "\n",
    "- TP: True positives (correctly predicted positive cases)\n",
    "- TN: True negatives (correctly predicted negative cases)\n",
    "- FP: False positives (incorrectly predicted positive cases)\n",
    "- FN: False negatives (incorrectly predicted negative cases)\n",
    "\n",
    "**Significance:**\n",
    "\n",
    "Accuracy is intuitive and easy to interpret, making it a common first metric for model evaluation. However, it can be misleading if the dataset is imbalanced (i.e., one class is much more frequent than the other). This is because a model can achieve high accuracy by simply predicting the majority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879be2c6-dd4a-4efb-be7a-32a21ec3ba9d",
   "metadata": {},
   "source": [
    "### 2. Precision\n",
    "\n",
    "**Definition:**\n",
    "Precision measures the proportion of positive predictions that are actually correct. It answers the question: \"Of all the samples that the model predicted as positive, how many were truly positive?\"\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Precision = $\\frac{TP}{TP + FP}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "Precision is crucial when the cost of a false positive is high. For example, in medical diagnosis, predicting a disease when it's not present (false positive) can lead to unnecessary treatments. In land classification, high precision means that when the model predicts a tile as agricultural, it is likely correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be151b6-8f48-4ec2-9cc9-0e7cf24feac5",
   "metadata": {},
   "source": [
    "### 3. Recall (sensitivity or true positive rate)\n",
    "\n",
    "**Definition:**\n",
    "Recall measures the proportion of actual positive cases that were correctly identified by the model. It answers: \"Of all the true positive samples, how many did the model identify?\"\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Recall = $\\frac{TP}{TP + FN}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "Recall is important when the cost of missing a positive case (false negative) is high. In land classification, high recall means the model is good at finding all the agricultural land, even if it sometimes mislabels non-agricultural land as agricultural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8eba49-ce14-4cef-9fb0-45c8a2a7931c",
   "metadata": {},
   "source": [
    "### 4. F1 score\n",
    "\n",
    "**Definition:**\n",
    "The F1 score is the harmonic mean of precision and recall. It provides a single metric that balances both concerns. It is especially useful when you need to find an equilibrium between precision and recall.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "F1 = $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "The F1 score is especially valuable when the class distribution is uneven or when both false positives and false negatives are important. It penalizes extreme values, so a model with high precision but low recall (or vice versa) will have a lower F1 score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab48236-1003-4888-b84c-bcd2b5b385e0",
   "metadata": {},
   "source": [
    "### 5. Confusion matrix\n",
    "\n",
    "**Definition:**\n",
    "A confusion matrix is a table that summarizes the performance of a classification algorithm. It displays the counts of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "|               | Predicted positive | Predicted negative |\n",
    "|---------------|-------------------|-------------------|\n",
    "| Actual positive | True positive (TP) | False negative (FN) |\n",
    "| Actual negative | False positive (FP) | True negative (TN) |\n",
    "\n",
    "**Significance:**\n",
    "The confusion matrix provides a detailed breakdown of model errors and successes, helping you understand not just how often the model is right, but *how* it is wrong. This is crucial for diagnosing issues like class imbalance or systematic misclassification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b5a8b-6836-4fb7-86db-36c6b8efb675",
   "metadata": {},
   "source": [
    "### 6. ROC-AUC (Receiver operating characteristic - Area under curve)\n",
    "\n",
    "**Definition:**\n",
    "ROC-AUC measures the model's ability to distinguish between classes across all possible classification thresholds. The ROC curve plots the true positive rate (recall) against the false positive rate at various thresholds. The AUC (area under the curve) summarizes this performance in a single value between 0 and 1.\n",
    "\n",
    "**Significance:**\n",
    "A model with an ROC-AUC of 1.0 perfectly distinguishes between classes, while a value of 0.5 suggests random guessing. ROC-AUC is especially useful for imbalanced datasets and when you care about the ranking of predictions rather than their absolute values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51947c1-a446-45f4-83d7-b9d8472847e4",
   "metadata": {},
   "source": [
    "## Import the evaluation metrics\n",
    "\n",
    "Here you define the functions to compute and print classification metrics including accuracy, precision, recall, F1 score, ROC-AUC, confusion matrix, and log loss. These functions support both Keras and PyTorch model outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab94b1-7072-4d1f-8838-cf14c99a17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             roc_curve, \n",
    "                             roc_auc_score,\n",
    "                             log_loss,\n",
    "                             classification_report,\n",
    "                             confusion_matrix,\n",
    "                             ConfusionMatrixDisplay,\n",
    "                            )\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# define a function to get the metrics comprehensively\n",
    "def model_metrics(y_true, y_pred, y_prob, class_labels):\n",
    "    metrics = {'Accuracy': accuracy_score(y_true, y_pred),\n",
    "               'Precision': precision_score(y_true, y_pred),\n",
    "               'Recall': recall_score(y_true, y_pred),\n",
    "               'Loss': log_loss(y_true, y_prob),\n",
    "               'F1 Score': f1_score(y_true, y_pred),\n",
    "               'ROC-AUC': roc_auc_score(y_true, y_prob),\n",
    "               'Confusion Matrix': confusion_matrix(y_true, y_pred),\n",
    "               'Classification Report': classification_report(y_true, y_pred, target_names=class_labels, digits=4),\n",
    "               \"Class labels\": class_labels\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "#function to print the metrics\n",
    "def print_metrics(y_true, y_pred, y_prob, class_labels, model_name):\n",
    "    metrics = model_metrics(y_true, y_pred, y_prob, class_labels)\n",
    "    print(f\"Evaluation metrics for the \\033[1m{model_name}\\033[0m\")\n",
    "    print(f\"Accuracy: {'':<1}{metrics[\"Accuracy\"]:.4f}\")\n",
    "    print(f\"ROC-AUC: {'':<2}{metrics[\"ROC-AUC\"]:.4f}\")\n",
    "    print(f\"Loss: {'':<5}{metrics[\"Loss\"]:.4f}\\n\")\n",
    "    print(f\"Classification report:\\n\\n  {metrics[\"Classification Report\"]}\")\n",
    "    print(\"========= Confusion Matrix =========\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=metrics[\"Confusion Matrix\"],\n",
    "                                  display_labels=metrics[\"Class labels\"])\n",
    "\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc53fe7",
   "metadata": {},
   "source": [
    "## Model download helper\n",
    "\n",
    "Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n",
    "You use `httpx` for asynchronous HTTP requests with error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e94bd9",
   "metadata": {},
   "source": [
    "## Model paths and download\n",
    "\n",
    "In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "keras_model_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/U-uPeyCyOQYh0GrZPGsqoQ/ai-capstone-keras-best-model-model.keras\"\n",
    "keras_model_name = \"ai-capstone-keras-best-model-model_downloaded.keras\"\n",
    "keras_model_path = os.path.join(data_dir, keras_model_name)\n",
    "\n",
    "pytorch_state_dict_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth\"\n",
    "pytorch_state_dict_name = \"ai_capstone_pytorch_best_model_state_dict_downloaded.pth\"\n",
    "pytorch_state_dict_path = os.path.join(data_dir, pytorch_state_dict_name)\n",
    "\n",
    "await download_model(keras_model_url, keras_model_path)\n",
    "await download_model(pytorch_state_dict_url, pytorch_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b6a821",
   "metadata": {},
   "source": [
    "## Dataset path and parameters\n",
    "\n",
    "Here, for downstream processing, you define \n",
    "1. the dataset directory path\n",
    "2. define image dimensions\n",
    "3. number of channels\n",
    "4. batch size\n",
    "5. number of classes\n",
    "6. class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "print(dataset_path)\n",
    "\n",
    "img_w, img_h = 64, 64\n",
    "n_channels = 3\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "\n",
    "agri_class_labels = [\"non-agri\", \"agri\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2bc67",
   "metadata": {},
   "source": [
    "## Keras model evaluation and prediction\n",
    "\n",
    "In this cell, you will:\n",
    "- Use `ImageDataGenerator` to rescale images.\n",
    "- Load test images from the dataset directory.\n",
    "- Load the saved Keras model using `tf.keras.models.load_model`.\n",
    "- Run predictions on the test set, collect predicted probabilities, predicted classes, and true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd7f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "prediction_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_w, img_h),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "keras_model = tf.keras.models.load_model(keras_model_path)\n",
    "\n",
    "steps = int(np.ceil(prediction_generator.samples / prediction_generator.batch_size))\n",
    "batch_size = int(prediction_generator.batch_size)\n",
    "print(f\"Number of Steps: {steps} with batch size: {batch_size}\")\n",
    "\n",
    "all_preds_keras = []\n",
    "all_probs_keras = []\n",
    "all_labels_keras = []\n",
    "\n",
    "for step_idx, step in enumerate(tqdm(range(steps), desc=\"Steps\")):\n",
    "    images, labels = next(prediction_generator)\n",
    "    preds = keras_model.predict(images, verbose='0')\n",
    "    all_probs_keras.extend(preds)\n",
    "    preds = (preds > 0.5).astype(int).flatten()\n",
    "    all_preds_keras.extend(preds)\n",
    "    all_labels_keras.extend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d28b7-43ac-49c0-ac58-4b13fcf76fff",
   "metadata": {},
   "source": [
    "#### Question: What does the code **`preds > 0.5`** in line `preds = (preds > 0.5).astype(int).flatten()` do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05026e2c-8187-4651-9dab-cbfe19b644ce",
   "metadata": {},
   "source": [
    "Please use the space below to write your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5ad27-832d-4f14-87d4-9ef5b856af62",
   "metadata": {},
   "source": [
    "\"It converts all predictions greater than 0.5 to True or assign to class 1. Rest of the predictions are False, assigned to class 0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008b4c1-2d7d-4a93-9714-a687b3e29a5e",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"It converts all predictions greater than 0.5 to True or assign to class 1. Rest of the predictions are False, assigned to class 0\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971389d2-f67f-4c21-a79a-2db089082422",
   "metadata": {},
   "source": [
    "## Keras metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec013b-f37f-4231-a33f-952650170d71",
   "metadata": {},
   "source": [
    "### Task 1: Print the performance metrics for the Keras model using `print_metrics` function\n",
    "\n",
    "Print various performance metrics for the **Keras** model. You may use the previously defined metrics print function `print_metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "print_metrics(y_true = all_labels_keras,\n",
    "              y_pred = all_preds_keras,\n",
    "              y_prob = all_probs_keras,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"Keras Model\"\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2e854-e1c3-4d08-a0a3-eaa09340a389",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_keras,\n",
    "              y_pred = all_preds_keras,\n",
    "              y_prob = all_probs_keras,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"Keras Model\"\n",
    "             )\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b613b4-911e-485c-9de7-f98b3c02e068",
   "metadata": {},
   "source": [
    "#### Question: What is the significance of `f1 score`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bfe96-2d87-46f1-9374-3d2755aae0c4",
   "metadata": {},
   "source": [
    "Please use the space below to write your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b7c86-c3a3-40ec-bc53-de6df17dea29",
   "metadata": {},
   "source": [
    "\"The F1 score is a single metric that balances precision (low false positives) and recall (low false negatives), making it ideal for evaluating models on imbalanced datasets.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e571bbc-2708-4db9-b9ca-6573c0bd095d",
   "metadata": {},
   "source": [
    "\n",
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"It is useful when both false positives and false negatives are important\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202c22e",
   "metadata": {},
   "source": [
    "## PyTorch model evaluation and prediction\n",
    "\n",
    "In this cell, you:\n",
    "- Set device for inference (GPU if available).\n",
    "- Define data transformations including resizing, normalization.\n",
    "- Load the dataset using `ImageFolder` and prepares a DataLoader.\n",
    "- Define the CNN architecture matching the saved state dict.\n",
    "- Load model weights.\n",
    "- Run inference on the test set, collecting predicted classes, probabilities, and true labels for metric calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d240b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Processing inference on {device}\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_w, img_h)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)\n",
    "test_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "    nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "    nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "    nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "    nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "    nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "    nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "    nn.Linear(1024, 2048), nn.ReLU(), nn.BatchNorm1d(2048), nn.Dropout(0.4),\n",
    "    nn.Linear(2048, num_classes)\n",
    ").to(device)\n",
    "\n",
    "print(\"Created model, now loading the weights from saved model state dict\")\n",
    "model.load_state_dict(torch.load(pytorch_state_dict_path))\n",
    "print(\"Loaded model state dict, now getting predictions\")\n",
    "\n",
    "all_preds_pytorch = []\n",
    "all_labels_pytorch = []\n",
    "all_probs_pytorch = []\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc=\"Step\")):\n",
    "#    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        probs = F.softmax(outputs, dim=1)[:, 1]  # probability for class 1\n",
    "        all_probs_pytorch.extend(probs.cpu())\n",
    "        all_preds_pytorch.extend(preds.cpu().numpy().flatten())\n",
    "        all_labels_pytorch.extend(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc86e4a-c572-439b-b23a-5e272299caf9",
   "metadata": {},
   "source": [
    "## PyTorch metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5119d485-8fbd-4c4e-9405-505369288e48",
   "metadata": {},
   "source": [
    "### Task 2: Print the performance metrics for the PyTorch model using `print_metrics`\n",
    "\n",
    "Print various performance metrics for the PyTorch model. You may use the previously defined metrics print function `print_metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "print_metrics(y_true = all_labels_pytorch,\n",
    "              y_pred = all_preds_pytorch,\n",
    "              y_prob = all_probs_pytorch,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"PyTorch Model\"\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c22ac-e1fa-489e-b886-8de9a7ff3848",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_pytorch,\n",
    "              y_pred = all_preds_pytorch,\n",
    "              y_prob = all_probs_pytorch,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"PyTorch Model\"\n",
    "             )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0a22b-3cf1-4543-b012-77c192042eff",
   "metadata": {},
   "source": [
    "#### Question: What are the total number of false negatives in the `confusion matrix` in the PyTorch model evaluated above? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf7d8f-a2c8-4e4f-9744-ccaf46354f70",
   "metadata": {},
   "source": [
    "Please use the space below to write your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b7d4a-463d-4b46-96d7-1e333f542933",
   "metadata": {},
   "source": [
    "\"Total number of false negatives in the confusion matrix in the PyTorch Model evaluated above is 5.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f876e-6165-4553-90f3-fcdd9bd1d056",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"Total Flase negatives are 5\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebfcf7",
   "metadata": {},
   "source": [
    "## ROC curve plotting\n",
    "\n",
    "First, define a function to plot ROC curves for binary or multi-class classification using scikit-learn's `roc_curve` and `roc_auc_score`. It handles both single-class and multi-class cases by binarizing labels if needed.\n",
    "\n",
    "Next, plot the ROC curves for both the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc(y_true, y_prob, model_name):\n",
    "    n_classes = y_prob.shape[1] if y_prob.ndim > 1 else 1\n",
    "    if n_classes == 1:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')\n",
    "    else:\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "            auc = roc_auc_score(y_true_bin[:, i], y_prob[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} class {i} (AUC = {auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45792448",
   "metadata": {},
   "source": [
    "### ROC curve plotting for both models\n",
    "\n",
    "Plot the ROC curves for both Keras and PyTorch models on the same figure for visual performance comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(np.array(all_labels_keras), np.array(all_probs_keras), \"Keras Model\")\n",
    "plt.show()\n",
    "plot_roc(np.array(all_labels_pytorch), np.array(all_probs_pytorch), \"PyTorch Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e6e57-9764-44a3-8566-cef177c1e684",
   "metadata": {},
   "source": [
    "## Comparing model performance\n",
    "\n",
    "Now compare the performance of different models to understand which model would be the best performer for your land classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871167c-5e80-4f64-bc89-106ffc63935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Keras model performance metrics\n",
    "metrics_keras = model_metrics(all_labels_keras, all_preds_keras, all_probs_keras, agri_class_labels)\n",
    "\n",
    "# get the PyTorch model performance metrics\n",
    "metrics_pytorch = model_metrics(all_labels_pytorch, all_preds_pytorch, all_probs_pytorch, agri_class_labels)\n",
    "\n",
    "\n",
    "# Display the comparison of metrics\n",
    "print(\"{:<18} {:<15} {:<15}\".format('\\033[1m'+ 'Metric' + '\\033[0m',\n",
    "                                    'Keras Model', \n",
    "                                    'PyTorch Model'))\n",
    "\n",
    "mertics_list = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "for k in mertics_list:\n",
    "    print(\"{:<18} {:<15.4f} {:<15.4f}\".format('\\033[1m'+k+'\\033[0m',\n",
    "                                              metrics_keras[k],\n",
    "                                              metrics_pytorch[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5367255-39c2-473d-95bb-393e8baf6622",
   "metadata": {},
   "source": [
    "### Metric analysis\n",
    "\n",
    "The metrics for the pre-trained Keras and PyTorch models for evaluating the provided dataset are:\n",
    "\n",
    "- **Accuracy**\n",
    "    1. Keras: 0.9925\n",
    "    2. PyTorch: 0.9988\n",
    "    \n",
    "    ===> Both models achieve exceptional accuracy, but the **PyTorch model makes fewer mistakes**.\n",
    "\n",
    "- **Precision**\n",
    "    1. Keras: 1.0000\n",
    "    2. PyTorch: 0.9983\n",
    "\n",
    "    ===> The **Keras** model perfectly **avoids false positives**, whereas the PyTorch model is slightly less perfect but still excellent.\n",
    "\n",
    "- **Recall**\n",
    "    1. Keras: 0.9850\n",
    "    2. PyTorch: 0.9993\n",
    "    \n",
    "    ===> The **PyTorch** model is marginally better at **identifying all true positives**, capturing nearly all actual positive cases, while the Keras model misses a few.\n",
    "\n",
    "- **F1 Score**\n",
    "    1. PyTorch: 0.9988\n",
    "    2. Keras: 0.9924\n",
    "    \n",
    "    ===> The F1 score, which balances precision and recall, favors the **PyTorch** model thanks to its **stronger recall**.\n",
    "\n",
    "- **ROC-AUC**\n",
    "    1. Keras: 1.0000\n",
    "    2. PyTorch: 1.0000\n",
    "    \n",
    "    ===> Both models reach maximum possible **discrimination between classes**, indicating outstanding capability for binary classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30912d-7ebe-4a8f-b5f1-d14fb1a5a12f",
   "metadata": {},
   "source": [
    "### **Model comparison: Key insights**\n",
    "\n",
    "\n",
    "**PyTorch model strengths**\n",
    "\n",
    " - Achieves the highest scores in accuracy, recall, and F1, indicating extremely robust overall performance and near-perfect classification of positive cases\n",
    "- ROC-AUC of 1.0 shows perfect class separability\n",
    "\n",
    "\n",
    "**Keras model strengths**\n",
    "\n",
    "- Displays almost perfect precision every positive prediction made is correct\n",
    "- Also achieves perfect ROC-AUC, indicating outstanding discrimination ability\n",
    "\n",
    "\n",
    "**Common strength**\n",
    "\n",
    "- Both models deliver flawless ROC-AUC, suggesting both are highly effective for this classification task\n",
    "\n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "Based on the scores from the uploaded pre-trained models:\n",
    "\n",
    "- The PyTorch model is preferable for applications where missing any positive instances is costly (higher recall)\n",
    "- The Keras model is optimal for scenarios where making any false positive error is unacceptable (higher precision).\n",
    "\n",
    "\n",
    "**Next**\n",
    "\n",
    "- Analyze the confusion matrices to investigate the errors.\n",
    "- Monitor real-world performance, as even marginal differences can become important in high-impact applications. \n",
    "\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Both models excel in all evaluated metrics and would be highly reliable in production. The PyTorch model demonstrates a modest edge in recall and F1 score, while the Keras model maximizes precision. The choice between models should ultimately reflect the specific requirements and risk tolerance of your use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb79672-3cdf-47b4-ae97-43d53965d480",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4ff47-d95c-400f-ae87-56ca79b2ba98",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully evaluated and compared two deep learning models, one using Keras and the other using the PyTorch framework.\n",
    "\n",
    "You learnt about a comprehensive workflow for comparing Keras and PyTorch models on the same dataset and got hands-on experience on:\n",
    "- data preparation\n",
    "- model loading\n",
    "- predicting dataset\n",
    "- metric computation\n",
    "- ROC visualization\n",
    "- Model performance comparison\n",
    "\n",
    "Using these framework independent metrics, you now know how to evaluate different models for their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5057e-a8f6-478d-8639-fd70fee4f8eb",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075dc2f-6ffa-45a6-b2d8-860217305244",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-14  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917371aa-f1b6-469e-b57f-cbb963d3eef7",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "a4d52cb879d68ff11658550d0ab0119df62da062a1361bfc915addcf5e1b237d"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
